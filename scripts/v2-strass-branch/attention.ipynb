{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854ba8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/neural/attention.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from models.base_classifier import BaseClassifier\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism for fMRI data\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention weights (batch_size, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        output = self.layer_norm(output + x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for sequence data\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with self-attention and feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Self-attention\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        output = self.layer_norm(ff_output + attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "class SelfAttentionClassifier(BaseClassifier):\n",
    "    \"\"\"Self-attention based classifier for fMRI data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def _create_model(self, input_dim: int, n_classes: int) -> nn.Module:\n",
    "        \"\"\"Create self-attention model\"\"\"\n",
    "        d_model = self.config.get('d_model', 256)\n",
    "        n_heads = self.config.get('n_heads', 8)\n",
    "        n_layers = self.config.get('n_layers', 6)\n",
    "        d_ff = self.config.get('d_ff', 1024)\n",
    "        dropout = self.config.get('dropout_rate', 0.1)\n",
    "        max_seq_len = self.config.get('max_seq_len', 1000)\n",
    "        \n",
    "        return SelfAttentionNetwork(\n",
    "            input_dim=input_dim,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            n_layers=n_layers,\n",
    "            d_ff=d_ff,\n",
    "            n_classes=n_classes,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "\n",
    "class SelfAttentionNetwork(nn.Module):\n",
    "    \"\"\"Complete self-attention network for fMRI classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, d_model: int, n_heads: int, n_layers: int,\n",
    "                 d_ff: int, n_classes: int, dropout: float = 0.1, max_seq_len: int = 1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, return_attention: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, input_dim) or (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        if len(x.shape) == 2:\n",
    "            # Reshape from (batch_size, input_dim) to (batch_size, 1, input_dim)\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Store attention weights\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x, attn_weights = transformer_block(x)\n",
    "            if return_attention:\n",
    "                all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Global pooling over sequence dimension\n",
    "        x = x.transpose(1, 2)  # (batch_size, d_model, seq_len)\n",
    "        x = self.global_pool(x).squeeze(-1)  # (batch_size, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, all_attention_weights\n",
    "        return logits\n",
    "\n",
    "class MLPClassifier(BaseClassifier):\n",
    "    \"\"\"Multi-Layer Perceptron classifier using PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def _create_model(self, input_dim: int, n_classes: int) -> nn.Module:\n",
    "        \"\"\"Create MLP model\"\"\"\n",
    "        hidden_dims = self.config.get('hidden_dims', [512, 256, 128])\n",
    "        dropout_rate = self.config.get('dropout_rate', 0.3)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, n_classes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'MLPClassifier':\n",
    "        \"\"\"Train MLP classifier\"\"\"\n",
    "        # Preprocess data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        y_tensor = torch.LongTensor(y_encoded).to(self.device)\n",
    "        \n",
    "        # Create model\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        n_classes = len(np.unique(y_encoded))\n",
    "        self.model = self._create_model(input_dim, n_classes).to(self.device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.get('learning_rate', 0.001),\n",
    "            weight_decay=self.config.get('weight_decay', 1e-4)\n",
    "        )\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.get('batch_size', 32),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        n_epochs = self.config.get('n_epochs', 100)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                print(f\"Epoch {epoch}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return self.label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return prediction probabilities\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def get_feature_importance(self) -> np.ndarray:\n",
    "        \"\"\"Return feature importance (gradient-based)\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before getting feature importance\")\n",
    "            \n",
    "        # Use gradient-based feature importance\n",
    "        dummy_input = torch.randn(1, self.scaler.n_features_in_).to(self.device)\n",
    "        dummy_input.requires_grad_(True)\n",
    "        \n",
    "        self.model.eval()\n",
    "        output = self.model(dummy_input)\n",
    "        output.backward(torch.ones_like(output))\n",
    "        \n",
    "        importance = torch.abs(dummy_input.grad).mean(dim=0).cpu().numpy()\n",
    "        return importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
