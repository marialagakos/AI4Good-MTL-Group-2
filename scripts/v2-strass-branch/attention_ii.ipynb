{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d6e016",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.classical.svm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any, Optional, Tuple\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_classifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseClassifier\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMultiHeadSelfAttention\u001b[39;00m(nn.Module):\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Multi-head self-attention mechanism for fMRI data\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Desktop\\AI4Good - Brain Project\\AI4Good-MTL-Group-2\\scripts\\v2-strass-branch\\models\\__init__.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_classifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseClassifier\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVMClassifier\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom_forest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogistic_regression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegressionClassifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\Desktop\\AI4Good - Brain Project\\AI4Good-MTL-Group-2\\scripts\\v2-strass-branch\\models\\classical\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# models/classical/__init__.py\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Classical machine learning classifiers.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVMClassifier\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom_forest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassical\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogistic_regression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegressionClassifier\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'models.classical.svm'"
     ]
    }
   ],
   "source": [
    "# models/neural/attention.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from models.base_classifier import BaseClassifier\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism for fMRI data\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention weights (batch_size, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        output = self.layer_norm(output + x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for sequence data\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with self-attention and feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Self-attention\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        output = self.layer_norm(ff_output + attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "class SelfAttentionClassifier(BaseClassifier):\n",
    "    \"\"\"Self-attention based classifier for fMRI data\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def _create_model(self, input_dim: int, n_classes: int) -> nn.Module:\n",
    "        \"\"\"Create self-attention model\"\"\"\n",
    "        d_model = self.config.get('d_model', 256)\n",
    "        n_heads = self.config.get('n_heads', 8)\n",
    "        n_layers = self.config.get('n_layers', 6)\n",
    "        d_ff = self.config.get('d_ff', 1024)\n",
    "        dropout = self.config.get('dropout_rate', 0.1)\n",
    "        max_seq_len = self.config.get('max_seq_len', 1000)\n",
    "        \n",
    "        return SelfAttentionNetwork(\n",
    "            input_dim=input_dim,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            n_layers=n_layers,\n",
    "            d_ff=d_ff,\n",
    "            n_classes=n_classes,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "\n",
    "class SelfAttentionNetwork(nn.Module):\n",
    "    \"\"\"Complete self-attention network for fMRI classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, d_model: int, n_heads: int, n_layers: int,\n",
    "                 d_ff: int, n_classes: int, dropout: float = 0.1, max_seq_len: int = 1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, return_attention: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, input_dim) or (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        if len(x.shape) == 2:\n",
    "            # Reshape from (batch_size, input_dim) to (batch_size, 1, input_dim)\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Store attention weights\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x, attn_weights = transformer_block(x)\n",
    "            if return_attention:\n",
    "                all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Global pooling over sequence dimension\n",
    "        x = x.transpose(1, 2)  # (batch_size, d_model, seq_len)\n",
    "        x = self.global_pool(x).squeeze(-1)  # (batch_size, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, all_attention_weights\n",
    "        return logits\n",
    "\n",
    "class MLPClassifier(BaseClassifier):\n",
    "    \"\"\"Multi-Layer Perceptron classifier using PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def _create_model(self, input_dim: int, n_classes: int) -> nn.Module:\n",
    "        \"\"\"Create MLP model\"\"\"\n",
    "        hidden_dims = self.config.get('hidden_dims', [512, 256, 128])\n",
    "        dropout_rate = self.config.get('dropout_rate', 0.3)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, n_classes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'SelfAttentionClassifier':\n",
    "        \"\"\"Train self-attention classifier\"\"\"\n",
    "        # Preprocess data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        y_tensor = torch.LongTensor(y_encoded).to(self.device)\n",
    "        \n",
    "        # Create model\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        n_classes = len(np.unique(y_encoded))\n",
    "        self.model = self._create_model(input_dim, n_classes).to(self.device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.get('learning_rate', 0.0001),\n",
    "            weight_decay=self.config.get('weight_decay', 1e-4)\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.config.get('n_epochs', 100)\n",
    "        )\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.get('batch_size', 32),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        n_epochs = self.config.get('n_epochs', 100)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch}/{n_epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return self.label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return prediction probabilities\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def get_feature_importance(self) -> np.ndarray:\n",
    "        \"\"\"Return attention-based feature importance\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before getting feature importance\")\n",
    "        \n",
    "        # Get attention weights from the model\n",
    "        dummy_input = torch.randn(1, self.scaler.n_features_in_).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, attention_weights = self.model(dummy_input, return_attention=True)\n",
    "        \n",
    "        # Average attention weights across all heads and layers\n",
    "        if attention_weights:\n",
    "            # attention_weights is a list of tensors (n_layers, batch_size, n_heads, seq_len, seq_len)\n",
    "            avg_attention = torch.stack(attention_weights).mean(dim=(0, 1, 2))  # Average over layers, batch, heads\n",
    "            # For feature importance, we use the attention to the input (first position)\n",
    "            feature_importance = avg_attention[0, :].cpu().numpy()\n",
    "            \n",
    "            # If sequence length is 1, create uniform importance\n",
    "            if len(feature_importance) == 1:\n",
    "                feature_importance = np.ones(self.scaler.n_features_in_) / self.scaler.n_features_in_\n",
    "            else:\n",
    "                # Pad or truncate to match input features\n",
    "                if len(feature_importance) < self.scaler.n_features_in_:\n",
    "                    feature_importance = np.pad(feature_importance, \n",
    "                                              (0, self.scaler.n_features_in_ - len(feature_importance)))\n",
    "                elif len(feature_importance) > self.scaler.n_features_in_:\n",
    "                    feature_importance = feature_importance[:self.scaler.n_features_in_]\n",
    "        else:\n",
    "            # Fallback to uniform importance\n",
    "            feature_importance = np.ones(self.scaler.n_features_in_) / self.scaler.n_features_in_\n",
    "            \n",
    "        return feature_importance\n",
    "    \n",
    "    def get_attention_weights(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get attention weights for input data\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before getting attention weights\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, attention_weights = self.model(X_tensor, return_attention=True)\n",
    "        \n",
    "        if attention_weights:\n",
    "            # Return attention weights from the last layer\n",
    "            return attention_weights[-1].cpu().numpy()\n",
    "        else:\n",
    "            return np.array([])\n",
    "\n",
    "# Complete the MLPClassifier implementation\n",
    "class MLPClassifier(BaseClassifier):\n",
    "    \"\"\"Multi-Layer Perceptron classifier using PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def _create_model(self, input_dim: int, n_classes: int) -> nn.Module:\n",
    "        \"\"\"Create MLP model\"\"\"\n",
    "        hidden_dims = self.config.get('hidden_dims', [512, 256, 128])\n",
    "        dropout_rate = self.config.get('dropout_rate', 0.3)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, n_classes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'MLPClassifier':\n",
    "        \"\"\"Train MLP classifier\"\"\"\n",
    "        # Preprocess data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        y_tensor = torch.LongTensor(y_encoded).to(self.device)\n",
    "        \n",
    "        # Create model\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        n_classes = len(np.unique(y_encoded))\n",
    "        self.model = self._create_model(input_dim, n_classes).to(self.device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.get('learning_rate', 0.001),\n",
    "            weight_decay=self.config.get('weight_decay', 1e-4)\n",
    "        )\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.get('batch_size', 32),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        n_epochs = self.config.get('n_epochs', 100)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                print(f\"Epoch {epoch}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return self.label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return prediction probabilities\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def get_feature_importance(self) -> np.ndarray:\n",
    "        \"\"\"Return feature importance (gradient-based)\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before getting feature importance\")\n",
    "            \n",
    "        # Use gradient-based feature importance\n",
    "        dummy_input = torch.randn(1, self.scaler.n_features_in_).to(self.device)\n",
    "        dummy_input.requires_grad_(True)\n",
    "        \n",
    "        self.model.eval()\n",
    "        output = self.model(dummy_input)\n",
    "        output.backward(torch.ones_like(output))\n",
    "        \n",
    "        importance = torch.abs(dummy_input.grad).mean(dim=0).cpu().numpy()\n",
    "        return importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
