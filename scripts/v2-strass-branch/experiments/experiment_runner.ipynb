{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ca6f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m project_root = Path(\u001b[34;43m__file__\u001b[39;49m).resolve().parents[\u001b[32m1\u001b[39m]\n\u001b[32m     16\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Import necessary modules from the project\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# experiments/experiment_runner.py\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Ensure the project root is in the path for imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path(__file__).resolve().parents[1]\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import necessary modules from the project\n",
    "from models import ClassifierFactory\n",
    "from data.loaders import MultimodalDataLoader\n",
    "#from utils.metrics import ModelEvaluator\n",
    "#from utils.io_utils import save_results, load_results\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Main experiment runner for classifier comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        self.data_loader = MultimodalDataLoader(self.config['data'])\n",
    "        #self.evaluator = ModelEvaluator()\n",
    "        #self.results = {}\n",
    "        \n",
    "    def run_experiment(self, experiment_name: str, subject_ids: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run a complete experiment\"\"\"\n",
    "        print(f\"Running experiment: {experiment_name}\")\n",
    "        \n",
    "        experiment_config = self.config['experiments'][experiment_name]\n",
    "        results = {\n",
    "            'experiment_name': experiment_name,\n",
    "            'classifiers': {},\n",
    "            'summary': {}\n",
    "        }\n",
    "        \n",
    "        # Load and prepare data\n",
    "        all_X, all_y = self._load_all_subjects_data(subject_ids)\n",
    "        \n",
    "        # Run each classifier\n",
    "        for classifier_config in experiment_config['classifiers']:\n",
    "            classifier_type = classifier_config['type']\n",
    "            print(f\"  Running classifier: {classifier_type}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            classifier_results = self._run_single_classifier(\n",
    "                classifier_type, \n",
    "                classifier_config['config'], \n",
    "                all_X, \n",
    "                all_y\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            classifier_results['runtime'] = end_time - start_time\n",
    "            results['classifiers'][classifier_type] = classifier_results\n",
    "            \n",
    "            print(f\"    Completed in {end_time - start_time:.2f} seconds\")\n",
    "            print(f\"    Mean CV Accuracy: {classifier_results['cv_scores'].mean():.4f} Â± {classifier_results['cv_scores'].std():.4f}\")\n",
    "        \n",
    "        # Generate summary\n",
    "        results['summary'] = self._generate_summary(results['classifiers'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _load_all_subjects_data(self, subject_ids: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load and concatenate data from all subjects\"\"\"\n",
    "        all_X = []\n",
    "        all_y = []\n",
    "        \n",
    "        for subject_id in subject_ids:\n",
    "            print(f\"  Loading data for subject: {subject_id}\")\n",
    "            X, _ = self.data_loader.load_fmri_data(subject_id)\n",
    "            y = self.data_loader.load_stimulus_labels(subject_id)\n",
    "            \n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "        \n",
    "        return np.vstack(all_X), np.hstack(all_y)\n",
    "    \n",
    "    def _run_single_classifier(self, \n",
    "                             classifier_type: str, \n",
    "                             classifier_config: Dict[str, Any],\n",
    "                             X: np.ndarray, \n",
    "                             y: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Run a single classifier with cross-validation\"\"\"\n",
    "        \n",
    "        # Create classifier\n",
    "        classifier = ClassifierFactory.create_classifier(classifier_type, classifier_config)\n",
    "        \n",
    "        # Cross-validation setup\n",
    "        cv_config = self.config['cross_validation']\n",
    "        cv = StratifiedKFold(\n",
    "            n_splits=cv_config['n_folds'],\n",
    "            shuffle=cv_config['shuffle'],\n",
    "            random_state=cv_config['random_state']\n",
    "        )\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        # Train final model on full dataset\n",
    "        classifier.fit(X, y)\n",
    "        \n",
    "        # Make predictions for detailed evaluation\n",
    "        y_pred = classifier.predict(X)\n",
    "        y_pred_proba = classifier.predict_proba(X)\n",
    "        \n",
    "        # Get feature importance\n",
    "        try:\n",
    "            feature_importance = classifier.get_feature_importance()\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not get feature importance: {e}\")\n",
    "            feature_importance = None\n",
    "        \n",
    "        return {\n",
    "            'classifier_type': classifier_type,\n",
    "            'config': classifier_config,\n",
    "            'cv_scores': cv_scores,\n",
    "            'mean_cv_score': cv_scores.mean(),\n",
    "            'std_cv_score': cv_scores.std(),\n",
    "            'classification_report': classification_report(y, y_pred, output_dict=True),\n",
    "            'confusion_matrix': confusion_matrix(y, y_pred).tolist(),\n",
    "            'feature_importance': feature_importance.tolist() if feature_importance is not None else None,\n",
    "            'model': classifier  # Store trained model\n",
    "        }\n",
    "    \n",
    "    def _generate_summary(self, classifier_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate experiment summary\"\"\"\n",
    "        summary = {\n",
    "            'best_classifier': None,\n",
    "            'best_accuracy': 0.0,\n",
    "            'classifier_ranking': []\n",
    "        }\n",
    "        \n",
    "        # Rank classifiers by cross-validation score\n",
    "        ranking = []\n",
    "        for clf_type, results in classifier_results.items():\n",
    "            ranking.append({\n",
    "                'classifier': clf_type,\n",
    "                'mean_accuracy': results['mean_cv_score'],\n",
    "                'std_accuracy': results['std_cv_score'],\n",
    "                'runtime': results['runtime']\n",
    "            })\n",
    "        \n",
    "        ranking.sort(key=lambda x: x['mean_accuracy'], reverse=True)\n",
    "        summary['classifier_ranking'] = ranking\n",
    "        \n",
    "        if ranking:\n",
    "            summary['best_classifier'] = ranking[0]['classifier']\n",
    "            summary['best_accuracy'] = ranking[0]['mean_accuracy']\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    #def save_experiment_results(self, results: Dict[str, Any], output_path: str):\n",
    "       # \"\"\"Save experiment results\"\"\"\n",
    "        #save_results(results, output_path)\n",
    "        #print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "    def run_hyperparameter_search(self, \n",
    "                                classifier_type: str, \n",
    "                                param_grid: Dict[str, List[Any]], \n",
    "                                X: np.ndarray, \n",
    "                                y: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Run hyperparameter search for a specific classifier\"\"\"\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        # Create base classifier\n",
    "        base_classifier = ClassifierFactory.create_classifier(classifier_type, {})\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            base_classifier,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        return {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'cv_results': grid_search.cv_results_\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
